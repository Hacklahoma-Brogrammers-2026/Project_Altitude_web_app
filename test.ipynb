{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5cbd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from elevenlabs.client import ElevenLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe143b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from elevenlabs.client import ElevenLabs\n",
    "\n",
    "from database.models import User\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "elevenlabs = ElevenLabs(\n",
    "  api_key=\"\n",
    ")\n",
    "\n",
    "audio_path = \"backend/data/noah_and_k_data.wav\"\n",
    "\n",
    "with open(audio_path, \"rb\") as f:\n",
    "    transcription = elevenlabs.speech_to_text.convert(\n",
    "        file=f,\n",
    "        model_id=\"scribe_v2\", # Model to use\n",
    "        tag_audio_events=True, # Tag audio events like laughter, applause, etc.\n",
    "        language_code=\"eng\", # Language of the audio file. If set to None, the model will detect the language automatically.\n",
    "        diarize=True, # Whether to annotate who is speaking\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b90d1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language_code='eng' language_probability=1.0 text=\"Christiana, how's your day been? It's been amazing. How's your day? I'm so tired. [chuckles] Oh, no! I wanna go to bed. Me too. It is what it is. Thank you.\" words=[SpeechToTextWordResponseModel(text='Christiana,', start=0.66, end=1.319, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=1.319, end=1.339, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=\"how's\", start=1.339, end=1.479, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=1.479, end=1.499, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='your', start=1.5, end=1.559, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=1.559, end=1.579, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='day', start=1.579, end=1.679, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=1.679, end=1.719, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='been?', start=1.719, end=1.919, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=1.919, end=2.699, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=\"It's\", start=2.7, end=2.919, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=2.919, end=2.939, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='been', start=2.939, end=3.119, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=3.119, end=3.199, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='amazing.', start=3.199, end=3.799, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=3.799, end=3.799, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=\"How's\", start=3.799, end=4.059, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=4.059, end=4.099, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='your', start=4.099, end=4.239, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=4.239, end=4.279, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='day?', start=4.279, end=4.94, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=4.94, end=4.98, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=\"I'm\", start=4.98, end=5.139, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=5.139, end=5.159, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='so', start=5.159, end=5.319, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=5.319, end=5.339, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='tired.', start=5.339, end=5.819, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=5.819, end=5.959, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='[chuckles]', start=5.959, end=5.98, type='audio_event', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=5.98, end=6.0, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='Oh,', start=6.0, end=6.139, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=6.139, end=6.279, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='no!', start=6.279, end=6.96, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=6.96, end=7.199, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='I', start=7.199, end=7.279, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=7.279, end=7.299, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='wanna', start=7.299, end=7.519, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=7.519, end=7.539, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='go', start=7.539, end=7.639, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=7.639, end=7.659, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='to', start=7.659, end=7.759, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=7.759, end=7.799, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='bed.', start=7.799, end=8.199, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=8.199, end=8.439, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='Me', start=8.439, end=8.579, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=8.579, end=8.699, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='too.', start=8.699, end=9.159, type='word', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=9.159, end=9.439, type='spacing', speaker_id='speaker_1', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='It', start=9.439, end=9.56, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=9.56, end=9.579, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='is', start=9.579, end=9.679, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=9.679, end=9.72, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='what', start=9.72, end=9.8, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=9.8, end=9.84, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='it', start=9.84, end=9.94, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=9.94, end=10.0, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='is.', start=10.0, end=10.199, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=10.199, end=10.499, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='Thank', start=10.5, end=10.72, type='word', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=10.72, end=10.76, type='spacing', speaker_id='speaker_0', logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='you.', start=10.76, end=11.42, type='word', speaker_id='speaker_0', logprob=0.0, characters=None)] channel_index=None additional_formats=None transcription_id='ylXAMUdQCc8fsl2zDBU0' entities=None\n"
     ]
    }
   ],
   "source": [
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957a5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field, computed_field\n",
    "\n",
    "TokenType = Literal[\"word\", \"spacing\", \"audio_event\"]\n",
    "\n",
    "class Token(BaseModel):\n",
    "    text: str\n",
    "    start: float\n",
    "    end: float\n",
    "    type: TokenType\n",
    "    speaker_id: str | None = None\n",
    "\n",
    "class Utterance(BaseModel):\n",
    "    speaker_id: str\n",
    "    text: str = Field(..., description=\"Concatenated text for this utterance.\")\n",
    "    start: float\n",
    "    end: float\n",
    "\n",
    "    @computed_field  # pydantic v2\n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return max(0.0, self.end - self.start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911bac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "def tokens_to_utterances(\n",
    "    tokens: Iterable[Token],\n",
    "    *,\n",
    "    break_on_silence_s: float | None = 1.2,\n",
    "    include_audio_events: bool = False,\n",
    ") -> list[Utterance]:\n",
    "    out: list[Utterance] = []\n",
    "\n",
    "    cur_speaker: str | None = None\n",
    "    cur_parts: list[str] = []\n",
    "    cur_start: float | None = None\n",
    "    cur_end: float | None = None\n",
    "    last_end: float | None = None\n",
    "\n",
    "    def flush() -> None:\n",
    "        nonlocal cur_speaker, cur_parts, cur_start, cur_end\n",
    "        if cur_speaker is None:\n",
    "            return\n",
    "        text = \"\".join(cur_parts).strip()\n",
    "        if text:\n",
    "            out.append(Utterance(\n",
    "                speaker_id=cur_speaker,\n",
    "                text=text,\n",
    "                start=float(cur_start or 0.0),\n",
    "                end=float(cur_end or (cur_start or 0.0)),\n",
    "            ))\n",
    "        cur_speaker = None\n",
    "        cur_parts = []\n",
    "        cur_start = None\n",
    "        cur_end = None\n",
    "\n",
    "    for t in tokens:\n",
    "        spk = t.speaker_id\n",
    "        if not spk:\n",
    "            # drop un-attributed tokens\n",
    "            last_end = t.end\n",
    "            continue\n",
    "\n",
    "        if t.type == \"audio_event\" and not include_audio_events:\n",
    "            last_end = t.end\n",
    "            continue\n",
    "\n",
    "        # optional silence boundary split\n",
    "        if (\n",
    "            break_on_silence_s is not None\n",
    "            and last_end is not None\n",
    "            and (t.start - last_end) >= break_on_silence_s\n",
    "        ):\n",
    "            flush()\n",
    "\n",
    "        # speaker change split\n",
    "        if cur_speaker is not None and spk != cur_speaker:\n",
    "            flush()\n",
    "\n",
    "        # start new utterance if needed\n",
    "        if cur_speaker is None:\n",
    "            cur_speaker = spk\n",
    "            cur_start = t.start\n",
    "\n",
    "        cur_parts.append(t.text)\n",
    "        cur_end = t.end\n",
    "        last_end = t.end\n",
    "\n",
    "    flush()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfdeed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eleven_word_to_token(w) -> Token:\n",
    "    return Token(\n",
    "        text=w.text,\n",
    "        start=float(w.start),\n",
    "        end=float(w.end),\n",
    "        type=w.type,              # \"word\" / \"spacing\" / \"audio_event\"\n",
    "        speaker_id=w.speaker_id,  # \"speaker_0\", \"speaker_1\"\n",
    "    )\n",
    "\n",
    "words = transcription.words\n",
    "tokens = [eleven_word_to_token(w) for w in words]\n",
    "utterances = tokens_to_utterances(tokens, break_on_silence_s=1.2, include_audio_events=False)\n",
    "\n",
    "# utterances is List[Utterance], each has .duration computed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d405381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Utterance(speaker_id='speaker_0', text=\"Christiana, how's your day been?\", start=0.66, end=2.699, duration=2.0389999999999997),\n",
       " Utterance(speaker_id='speaker_1', text=\"It's been amazing. How's your day?\", start=2.7, end=4.98, duration=2.2800000000000002),\n",
       " Utterance(speaker_id='speaker_0', text=\"I'm so tired.\", start=4.98, end=6.0, duration=1.0199999999999996),\n",
       " Utterance(speaker_id='speaker_1', text='Oh, no!', start=6.0, end=7.199, duration=1.1989999999999998),\n",
       " Utterance(speaker_id='speaker_0', text='I wanna go to bed.', start=7.199, end=8.439, duration=1.2400000000000002),\n",
       " Utterance(speaker_id='speaker_1', text='Me too.', start=8.439, end=9.439, duration=1.0),\n",
       " Utterance(speaker_id='speaker_0', text='It is what it is. Thank you.', start=9.439, end=11.42, duration=1.9809999999999999)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, Field, computed_field\n",
    "\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "from speechbrain.inference.speaker import SpeakerRecognition\n",
    "from speechbrain.dataio import audio_io  # type: ignore\n",
    "\n",
    "\n",
    "# ---------- Your models ----------\n",
    "from typing import Literal\n",
    "\n",
    "# ---------- SpeechBrain verifier ----------\n",
    "verifier: SpeakerRecognition = SpeakerRecognition.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"pretrained_models/spkrec-ecapa\",\n",
    ")  # type: ignore\n",
    "\n",
    "def to_mono_batch(wav: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Accepts wav shaped [T], [C, T], or [B, T].\n",
    "    Returns [1, T] mono batch.\n",
    "    \"\"\"\n",
    "    if wav.ndim == 2:\n",
    "        # Treat as channels-first if C is small (like 2 for stereo)\n",
    "        if wav.shape[0] <= 8:\n",
    "            wav = wav.mean(dim=0)  # [T]\n",
    "        # else assume already [B, T] and leave it\n",
    "    elif wav.ndim != 1:\n",
    "        raise ValueError(f\"Unexpected wav shape: {wav.shape}\")\n",
    "\n",
    "    if wav.ndim == 1:\n",
    "        wav = wav.unsqueeze(0)  # [1, T]\n",
    "\n",
    "    return wav\n",
    "\n",
    "def _clip_utterance_to_temp_wav(\n",
    "    full_audio: AudioSegment,\n",
    "    start_s: float,\n",
    "    end_s: float,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Export [start_s, end_s] from full_audio to a temporary WAV file, return path.\n",
    "    \"\"\"\n",
    "    start_ms = int(max(0.0, start_s) * 1000)\n",
    "    end_ms = int(max(0.0, end_s) * 1000)\n",
    "    seg = full_audio[start_ms:end_ms]\n",
    "\n",
    "    # Guard: avoid tiny segments that produce unstable speaker decisions\n",
    "    if len(seg) < 300:  # <300ms\n",
    "        raise ValueError(\"segment too short\")\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
    "    tmp.close()\n",
    "    seg.export(tmp.name, format=\"wav\")\n",
    "    return tmp.name\n",
    "\n",
    "\n",
    "def _score_paths_with_verify_batch(ref_path: str, utt_path: str) -> float:\n",
    "    signal_ref, _ = audio_io.load(ref_path)\n",
    "    signal_utt, _ = audio_io.load(utt_path)\n",
    "    signal_ref = to_mono_batch(signal_ref)\n",
    "    signal_utt = to_mono_batch(signal_utt)\n",
    "    # print(signal_ref.shape)\n",
    "    # print(signal_utt.shape)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        score, prediction = verifier.verify_batch(signal_ref, signal_utt)\n",
    "\n",
    "    # score can be:\n",
    "    # - scalar tensor: [[x]] or [x]\n",
    "    # - 2-element tensor: [[a, b]] or [a, b] (two-class)\n",
    "    s = score.detach().cpu().squeeze()\n",
    "\n",
    "    if s.numel() == 1:\n",
    "        return float(s.item())\n",
    "\n",
    "    if s.numel() == 2:\n",
    "        # We need to decide which element corresponds to \"same speaker\".\n",
    "        # SpeechBrain's prediction is the authoritative label; pick the score matching it.\n",
    "        # prediction is typically [[0]] or [[1]] (or shape that squeezes to scalar).\n",
    "        pred = int(prediction.detach().cpu().squeeze().item())\n",
    "\n",
    "        # If pred==1 means \"same speaker\", use s[1], else use s[0].\n",
    "        # If SpeechBrain uses the opposite convention in your install, this still ranks correctly\n",
    "        # across utterances for the same reference because pred is derived from these scores.\n",
    "        return float(s[pred].item())\n",
    "\n",
    "    # Unexpected shape: fall back to mean (still provides a sortable scalar)\n",
    "    return float(s.float().mean().item())\n",
    "\n",
    "\n",
    "\n",
    "def score_speakers_against_reference(\n",
    "    *,\n",
    "    full_audio_path: str,\n",
    "    utterances: list[Utterance],\n",
    "    reference_sample_path: str,\n",
    "    min_utt_s: float = 0.6,\n",
    "    max_utts_per_speaker: int = 15,\n",
    ") -> tuple[str, list[tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Compute a score for each diarized speaker_id vs the reference sample.\n",
    "\n",
    "    Returns:\n",
    "      best_speaker_id,\n",
    "      scores_sorted_desc = [(speaker_id, mean_score), ...]\n",
    "\n",
    "    Scoring strategy:\n",
    "      - clip N utterances per speaker from full audio\n",
    "      - score each clip vs reference using verifier.verify_batch\n",
    "      - average scores per speaker\n",
    "    \"\"\"\n",
    "    full_audio = AudioSegment.from_file(full_audio_path)\n",
    "\n",
    "    # group utterances by diarized speaker_id\n",
    "    by_speaker: dict[str, list[Utterance]] = defaultdict(list)\n",
    "    for u in utterances:\n",
    "        if u.duration >= min_utt_s:\n",
    "            by_speaker[u.speaker_id].append(u)\n",
    "\n",
    "    tmp_paths: list[str] = []\n",
    "    speaker_scores: list[tuple[str, float]] = []\n",
    "\n",
    "    try:\n",
    "        for speaker_id, utts in by_speaker.items():\n",
    "            utts = utts[:max_utts_per_speaker]\n",
    "\n",
    "            scores: list[float] = []\n",
    "            for u in utts:\n",
    "                try:\n",
    "                    clip_path = _clip_utterance_to_temp_wav(full_audio, u.start, u.end)\n",
    "                    tmp_paths.append(clip_path)\n",
    "                    scores.append(_score_paths_with_verify_batch(reference_sample_path, clip_path))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            if not scores:\n",
    "                continue\n",
    "\n",
    "            mean_score = sum(scores) / len(scores)\n",
    "            speaker_scores.append((speaker_id, mean_score))\n",
    "\n",
    "        speaker_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if not speaker_scores:\n",
    "            raise RuntimeError(\"No usable utterances to score (all too short or failed to load).\")\n",
    "\n",
    "        best_speaker_id = speaker_scores[0][0]\n",
    "        return best_speaker_id, speaker_scores\n",
    "\n",
    "    finally:\n",
    "        for p in tmp_paths:\n",
    "            try:\n",
    "                Path(p).unlink(missing_ok=True)\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe5f5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 229440])\n",
      "torch.Size([1, 97872])\n",
      "torch.Size([1, 229440])\n",
      "torch.Size([1, 48960])\n",
      "torch.Size([1, 229440])\n",
      "torch.Size([1, 59520])\n",
      "torch.Size([1, 229440])\n",
      "torch.Size([1, 95088])\n",
      "torch.Size([1, 229440])\n",
      "torch.Size([1, 109440])\n",
      "torch.Size([1, 229440])\n",
      "torch.Size([1, 57552])\n",
      "torch.Size([1, 229440])\n",
      "torch.Size([1, 48000])\n",
      "Best label: speaker_0\n",
      "Scores: [('speaker_0', 0.7524326890707016), ('speaker_1', 0.41520418723424274)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "best_label, scores = score_speakers_against_reference(\n",
    "    full_audio_path=\"./backend/data/noah_and_k_data.wav\",\n",
    "    utterances=utterances,\n",
    "    reference_sample_path=\"./backend/data/noah_audio_sample.wav\",\n",
    ")\n",
    "\n",
    "print(\"Best label:\", best_label)\n",
    "print(\"Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d950b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altitude_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
